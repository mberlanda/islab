# Incontro ISLAB 2017-04-20

### Riduzione delle componenti

- ridurre il volume dei dati che si vogliono analizzare
- preservare le features più significative
- può eliminare il rumore

Possiamo valutare le covarianza (es. pca) oppure definire delle feature più rappresentative.

Oggi facciamo una carrellata di alcuni algoritmi di clustering:

http://scikit-learn.org/

Affinity Propagation Clustering
- Frey, Brendon and Delbert Dueck, _Clustering by passing messages between data points_
- si bassa sullo scambio di messaggi tra i punti
  - responsibility: `r(i, k)` da i verso k verificare quanto sia probabile che k sia un exemplar di i
    - similarità tra i e k - massimo valore della somma tra availability e responsability di ciascun k' diverso da k
    - se esiste un punto più simile a k, allora r è negativo, altrimenti positivo
  - availability: `a(i, k)` da k verso i verificare quanto sia probabile che i sia un exemplar di k
    - valore minimo tra 0 e la responsibility (r(k, k)) di k rispetto a sé stesso + la sommatoria del massimo tra 0 e la responsibility per ciascun i' diverso da i e k, e k
    - se aumentiamo il valore di k,k possiamo renderlo maggiormento autoresponsible
    - a(k, k) sommatoria del massimo tra 0 e r(i',k) per ciascun i' divers da k
  - inizialmente settiamo tutte le availability a 0 e iteriamo sui responsibility e availability
- ciò che ha maggior influenza è la self availability
- nei primi cicli c'è un salto numerico molto forte che si possono limitare con un lambda dumping factor che pondera i risultati
- per decidere i cluster procediamo come segue:
  - prendiamo per ogni punto i prendiamo il valore di k che massimizza a(i,k)+r(i,k)
- http://science.sciencemag.org/content/315/5814/972.full
- Vantaggi:
  - non richiede di settare k
  - possiamo privilegiare alcuni documenti attribuendo una self similarity > 1
  - abbiamo un rappresentante per ogni cluster (non un centroide virtuale ma un documento rappresentativo)

Mean-shift
- può essere inizializzato con dei seed
- non ha bisogno di un numero a priori di cluster
- sposta i centroidi della media della distanza tra gli elementi del cluster


Spectral clustering
- costruzione della matrice https://en.wikipedia.org/wiki/Laplacian_matrix
  - matrice di affinità
  - grafo laplaciano
  - differenza tra matrici D - A, dove D è la degree matrix
- definiamo i k cluster
  - prendiamo i primi k autovettori della matrice (eigenvectors)
    - ogni riga è un punto/documento
    - ogni colonna è il k-esimo eigenvector
  - eseguiamo k-means

DBSCAN
- concepisce i cluster come aree dense di documenti
- classificazione
  - core-point: punto che ha almeno m punti che stanno all'interno di una distanza e
- input:
  - m punti minimi all'interno di una distanza e
- possiamo avere outlier residuali
- processo
  - prendo un punto e verifico se ha almeno m elementi a distanza e
    - se no, lo marco come rumore
    - se si, ripeto l'operazione su tutti i punti che ne fanno parte

Birch
- molto complesso gestendo i nodi


Omessi tutti i metodi probablistici

> Se non siamo soddisfatti dei nostri cluster, possiamo applicare un kernel per aumentare le differenze (contrario della pca). Concetto da sviluppare

Se ho dei punti che non sono linearmente separabili nello spazio, non riesco a distinguere un cluster che può apparire come enclave.
Invece di costruire un separatore complicato, aggiungo un'altra dimensione. Il kernel è la funzione che aggiunge una dimensione

Popular Kernels:
- linear
- gaussian
- exponential
- polynomial
- ...



Valutazione del clustering

- Purity: per ogni cluster prodotto dal primo dei due metodi, calcolo la massima intersezione di un cluster prodotto con un altro metodo. Per ogni cluster prendo quello che ha la massima intersezione e faccio una media. Valuta la precision del mio algoritmo, ma non se aggrega come dovrebbe.
- Normalized Mutual Information
- Rand coefficient: prendo i punti a due a due N(N-1)/2 gruppi di coppie di oggetti nel dataset. La true negative è piuttosto importante quindi è spesso vicino all'uno. Quindi meglio:
  - precision P = TP / TP + FP
  - recall R = TP / TP + FN
  - f-measure Fbeta = (beta^2 +1) PR/ beta^2(P+R) dove beta è solitamente 2

Mutual information: un evento si verifica con probabilità superiore alla probabilità generica di quell'evento.

http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation
