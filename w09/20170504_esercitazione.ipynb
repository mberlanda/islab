{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampling: esercitazione\n",
    "\n",
    "Esempio tratto da [Ediwn Chen Blog](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First, create a document sets\n",
    "docs = [\n",
    "    ['apple', 'ios', 'mac', 'book'],\n",
    "    ['apple', 'mac', 'book', 'apple', 'store'],\n",
    "    ['banana', 'mango', 'fruit'],\n",
    "    ['apple', 'fruit'],\n",
    "    ['orange', 'strawberry'],\n",
    "    ['apple', 'fruit', 'mac', 'ios']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per LDA dobbiamo considerare ogni occorrenza delle parole all'interno di un topic, non il termine in sé ma in ciascuna istanza.\n",
    "\n",
    "Per farlo possiamo dare un ID univoco a ogni documento e a ogni termine di ogni documento.\n",
    "E.g. 'apple' nel secondo documento è 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decidiamo un numero fisso k di topics\n",
    "\n",
    "K = 2\n",
    "topics = dict([(i, [])for i in range(0,K)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simplified collapsed Gibbs sampling\n",
    "\n",
    "# Go through each document, and randomly assign each word in the document to one of the K topics.\n",
    "docs_pos = []\n",
    "for i, d in enumerate(docs):\n",
    "    d_pos = []\n",
    "    for w in d:\n",
    "        t = np.random.choice(topics.keys())\n",
    "        d_pos.append(t)\n",
    "        topics[t].append(w)\n",
    "    docs_pos.append(d_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['mac', 'book', 'book', 'strawberry', 'apple', 'ios']\n",
      "1 ['apple', 'ios', 'apple', 'mac', 'apple', 'store', 'banana', 'mango', 'fruit', 'apple', 'fruit', 'orange', 'fruit', 'mac']\n"
     ]
    }
   ],
   "source": [
    "for k, words in topics.items():\n",
    "    print k, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'ios', 'mac', 'book']\n",
      "[1, 1, 0, 0]\n",
      "['apple', 'mac', 'book', 'apple', 'store']\n",
      "[1, 1, 0, 1, 1]\n",
      "['banana', 'mango', 'fruit']\n",
      "[1, 1, 1]\n",
      "['apple', 'fruit']\n",
      "[1, 1]\n",
      "['orange', 'strawberry']\n",
      "[1, 0]\n",
      "['apple', 'fruit', 'mac', 'ios']\n",
      "[0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print d\n",
    "    print docs_pos[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).\n",
    "\n",
    "Ovvero adesso abbiamo ottenuto z, pertanto implicitamente abbiamo anche fi e theta. Assumo che alfa e beta siano tutti equiprobabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta(doc, topics, topic):\n",
    "    doc_topics = {}\n",
    "    for k, t_words in topics.items():\n",
    "        doc_topics[k] = len([x for x in doc if x in t_words])\n",
    "    return float(doc_topics[topic])/sum(doc_topics.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print theta(docs[1], topics, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def phi(topics, word):\n",
    "    word_topics = {}\n",
    "    for k, t_words in topics.items():\n",
    "        w_counter = dict(Counter(t_words).most_common())\n",
    "        try:\n",
    "            word_t = w_counter[word]\n",
    "        except KeyError:\n",
    "            word_t = 0\n",
    "        word_topics[k] = word_t\n",
    "    probs = np.zeros(len(word_topics))\n",
    "    for i, k in enumerate(sorted(word_topics.keys())):\n",
    "        probs[i] = float(word_topics[k]) / sum(word_topics.values())\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print phi(topics, 'fruit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So to improve on them, for each document d…\n",
    "  - Go through each word w in d…\n",
    "    - And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word’s topic with this probability). (Also, I’m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)\n",
    "    - In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.\n",
    "\n",
    "\n",
    "Tre possibilità sulle probabilità:\n",
    "\n",
    "- normalizzo il vettore `probs` a somma 1\n",
    "- ottengo un valore di una distribuzione di Dirichlet `np.random.dirichlet(probs)`\n",
    "- genero un rand entro il max di probs e prendo il valore corrispondente\n",
    "\n",
    "```\n",
    "print w, 'old to', old_assignment, 'new to', new_assignment\n",
    "print w, p_norm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random(data):\n",
    "    data = sorted([(i, x) for i, x in enumerate(data)], key=lambda y: -y[1])\n",
    "    r = np.random.uniform()\n",
    "    s = 0\n",
    "    for pos, value in data:\n",
    "        s+= value\n",
    "        if r <= s:\n",
    "            break\n",
    "    return pos\n",
    "        \n",
    "def pseudo_gibbs(docs, topics, pos):\n",
    "    for i, doc in enumerate(docs):\n",
    "        for j, w in enumerate(doc):\n",
    "            probs = []\n",
    "            for k in sorted(topics.keys()):\n",
    "                p_topic_doc = theta(doc, topics, k)\n",
    "                p_word_topic = phi(topics, w)[k]\n",
    "                probs.append(p_topic_doc * p_word_topic)\n",
    "            p_norm = np.array(probs)/sum(probs)\n",
    "            old_assignment = pos[i][j]\n",
    "            new_assignment = get_random(p_norm)\n",
    "            topics[old_assignment].remove(w)\n",
    "            topics[new_assignment].append(w)\n",
    "            pos[i][j] = new_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6  0.4]\n",
      "[ 0.2  0.8]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n",
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    pseudo_gibbs(docs, topics, docs_pos)\n",
    "    print phi(topics, 'apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [('book', 2), ('ios', 2), ('strawberry', 1)]\n",
      "1 [('apple', 5), ('mac', 3), ('fruit', 3), ('mango', 1), ('orange', 1), ('banana', 1), ('store', 1)]\n"
     ]
    }
   ],
   "source": [
    "for k, words in topics.items():\n",
    "    print k, Counter(words).most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
