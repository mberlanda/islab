# Incontro ISLAB 2017-05-04

La scorsa lezione abbiamo visto un approccio che prevede la scomposizione della matrice termine-documento `mxn` in tre componenti (_latent semantic indexing_):

- matrice dei topic `kxk` (U)
- matrice termini-topic `mxk` (Signa)
- matrice documenti-topic `nxk` (Vt)

Possiamo usare questa cosa per due obiettivi:

- query
- soft-clustering

L'approcio probabilistico presenta vantaggi rispetto all'LSI in termini di significatività della contribuzione di una risorsa a un topic e in termini computazionali.

### Latent Dirichlet allocation (LDA)

Si tratta di un modello probabilistico non supervisionato.

L'approccio muove dal presupposto che il set di partenza sia creato da un generatore bayesiano (ciascuno con le proprie probabilità).

Se assumiamo che i testi di un corpus siano generati da un generatore probabilisti, invertendo il processo possiamo individuare la distribuzione delle probabilità dei termini osservandone il risultato.

LSA prende la matrice termini documenti e la scompone in tre matrici di cui sopra.

LDA considera che la matrice termini documenti sia generata da un generatore probabilistico che si basa su _fi_ (probabilità del termine in un topic) e _theta_ (probabilità di un topic di aver generato il documento).

Differenze:

- non abbiamo una matrice riferita unicamente ai topic

Per comprendere il meccanismo partiamo prima dalle due distribuzione per risalire al posterior (documenti-termini) in un processo di inferenza bayesiana.

Anche in questo caso si tratta di soft-clustering.

### Il modello generativo della LDA

Supponiamo di avere già le distribuzioni fi e theta.

Possiamo fare una sorta di T9 semantico. Possiamo, data una nuova immagine, valutarne la probabilità di appartenere a un topic.

Immaginiamo di avere:

- _fiK_ probabilità discreta dei termini rispetto al topic K
- _thetaD_ distribuzione dei documenti sui topic
- _zi_ indice dei topic per la parola _wi_
- _alfa_ e _beta_ iperparametri per la generazione di una distribuzione di Dirichlet


_Stimare le probabilità_

quante sono le probabilità che un documento contenga un termine? `N!`

utilizzando l'approccio delle bag of words, a noi non interessa l'ordine dei tag = N! / (N-n)n! = (N n) coefficiente binomiale

coefficiente binomiale: permutazioni possibili n su un N

distribuzione binomiale di un evento, considera

[Dirichlet_distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)

Generata a partire da un certo parametro e stabilisce le probabilità di una serie di eventi.

fiK ~ Dir(beta) => lunghezza vettore k, somma 1
thetaK ~ Dir(alfa) => lunghezza vettore k, somma 1

Come faccio a generare un documento?

Per generare un documento definisco la lunghezza (anche casuale) e ne genero le parole.

Generatore bayesiano (T9):

- prendo una prima parola
- prendo una seconda parola condizionata dalla probabilità della prima etc.

Nel nostro caso ho anche i topic:

- sulla base della distribuzione dei documenti dei topic thetaD, scelgo un topic
- all'interno del topic faccio il processo di cui sopra

zi ~ Discrete(thetaD)
wi ~ Discrete(fiZi)

Generatore casuale con una distribuzione probabilistica varia:

```py
import numpy as np

a = np.array([0.5, 0.3, 0.1, 0.1])
r = np.random.uniform() #genero un numero casuale e lo confronto con la probabilità
# ordino le probabilità
# prendo il numero r tra 0 e 1 e prendo l'indice di cui r è minore
```

```py
import numpy as np

alpha = np.array([0.5, 0.5])
np.random.dirichlet(alpha)
# alpha mi indica quanto le probabilità di un oggetto debbano essere sulperiori ad un altro
# non devono sommare a 1
# il parametro determina la shape della funzione
alpha = np.array([5, 0.5])

```

Quello che abbiamo in mano è l'ipotetico prodotto di questo generatore. Non non vogliamo generare i documenti ma vogliamo risalire alle distribuzioni che le hanno create.

### Modellazione del problema

- w è la distribuzione delle parole
- z sono i topic
- theta distribuzione documenti sui topic
- fi distribuzione delle parole sui topic

(w, z, theta, fi | alfa, beta)

dipendono tutti da alfa e beta

usando il teorema di Bayes:

p(w, z, theta, fi | alfa, beta) = p(fi| beta) p(theta | alpha) p(z| theta) p (w| fiz)

Partendo dai dati dobbia stimare z (i topic), fi e theta. I tag li conosciamo già (w).

thetaD rappresentazione di un documento nello spazio di un topic
zi rappresenta l'istanza di una paraola in wi
ogni fiK è una matrice K X W doce fi ij = p(wi|zj)

Inferenza a posteriori

di per sé è intrattabile perciò si usa una tecnica di sampling.

**Gibbs sampling**

Markov Chain Monte Carlo (MCMC) framework. Ogni step è condizionato dal precedente.

Dobbiamo stimare la probabilità randomica di p(X) = (px1,...,pxn):

- inizializzo X random
- processo iteraivo per ogni step
  - xt+1 1 ~ p(x1 | xt2, xt3, ..., xtn)
  - xt+1 2 ~ p(x2 | xt+1 1, xt3, ..., xtn)
  - ...
  - xt+1 n ~ p(xn | xt+1 1, xt+1 2, ..., xt+1 n-1)
- a un certo punto converge

Per stimare theta dzi = numero di parole assegnate a zeta + alfai / somma del numero di tutte le parole assegnate a tutti i documenti + alfai

Se alfa è molto alto tenderà a dare 1

fi zwi = numero di volte che la parola w è assegnata al topic z + beta / numero ti tute le altre parole assegnate a zeta + beta

Se ho z ( la distribuzione sui topic), posso usarla per la distribuzione di probailità e quindi non devo stimarle.


Collapsed Gibbs sampling: collasso le due stime più difficile sull'assegnamento dei topic

p(zi | z non-i, alfa, beta, w)

si scompone nuovamente con il teorema di bayes e alla fine devo stimare

p(z,w| alfa, beta) = integrale doppio p(zi, w, fi, theta| alfa, beta) dfi dtheta

- scompongo con un prodotto di probabilità a due a due come sue
- scompongo il prodotto di integrali


Int p(z|theta)p(theta|a)dtheta * Int p(w|fiz)p(phi|beta)dhi

Risorse:

- [http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)


```py
from sklear.decomposition import LatentDirichletAllocation

LDA = LatentDirichletAllocation(n_topics=2, learning_method='batch').fit(M.T)
```

Come determinare il numero di topic?

[https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process](https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process)


### Ultime considerazioni

Caldeggiamente consigliato l'utilizzo del paccketto `gensim`

[https://radimrehurek.com/gensim/tutorial.html](https://radimrehurek.com/gensim/tutorial.html)

Vantaggi:

- streaming degli oggetti in memoria
- tutte le classi sono pensate per fare facilmente topic modelin

### Esercizi

L'idea è quella di lavorare sul topic modeling:

- individuare i topic
- assegnarli alle risorse
- usarli per clusterizzare

Attualmente usiamo docs e words, ma sarebbe bello introdurre altri elementi: tempo, engagement etc. facendo una mistura più adeguata

Guardare gensin e provare a lavorare sui nostri dataset per comparare i risultati che possiamo ottenere dai vari algoritmi di clustering con il topic modeling (HDP)

Se il numero di topic non è ottimale, sono presenti moltissimi topic che si sovrappongo pesantemente (provare a fonderlo)

Topic Modeling Perplexity
http://qpleple.com/perplexity-to-evaluate-topic-models/

Analisi cross-datasets
