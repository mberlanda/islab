# Incontro ISLAB 2017-04-13

### Hierarchical Clustering

- Produce una struttura gerarchica dove la relazione è il contenimento
- Non richiedere di definire un numero di cluster, ma possiamo influenzarne il numero
- Nella maggior parte delle implementazioni è deterministico
- La complessità è almeno quadrica rispetto al numero di documenti

Due famiglie di algoritmi di clustering:

- `Bottom-up or agglomerative clustering`: ogni documento è considerato inizialmente come singleton-cluster
- `Bottom-down or divisive clustering`: tutta la collezione è considerata inizialmente come singleton-cluster

**Dendogramma**: albero di affinità. Non è un insieme di clusters, ma un albero che collega le risorse.

- Prendo la matrice quadrata delle distanze
- Considero le due colonne più simili e le aggrego => cancellare le righe|collone che sto rappresentando sostituiendole con l'aggregato delle due
- Ripeto l'operazione aggregando punti o cluster già formati finché arriva a uno

Non si tratta di una clusterizzazione, ma di un'organizzazione gerarchica delle risorse. A seconda di dove _taglio_ il dendogramma ad un certo livello di distanza e ne definisco la quantità (es. max distanza 0.11). Più elevata è la distanza avrò cluster meno omogenei ma più popolosi.

Dove taglio K:

- decido sulla base del livello di similarity
- definisco K e analizzo il dendogramma in maniera da ottenere K clusters
- Applicare K = argminK' [RSS(K')+lK] dove K' è il cut treshold, distanza residuale nei cluster che ho formato, l pesa il numero di clusters
  - la distanza del salto di un'aggregazione alla successiva

Implementazione Naive:

- calcolo la matrice quadrata NxN per inserire le similarità
- per ogni n-1 unisco righe e colonne più simili
- mantengo la struttura gradualmente

Strategie per la Cluster Similarity

- single link: la distanza minima
- complete link: la distanza massima
- average link: la media tra la distanza punto a punto di tutti i documenti
- weighted link
- centroid link: calcolo la distanza tra i centroidi di due cluster
- median link: la media dei due centroidi
- ward link



   |  A  |   B | C
---|-----|-----|-----
 A | 0   | 0.2 | 0.4
 B |     |  0  | 0.6
 C |     |     | 0

   | CL1 |  C
---|-----|-----
CL1| 0   | 0.2
 C |     |  0


Per due punti conosco diversi modi come calcolare la distanza.

Come calcolare la distanza tra due cluster

```
Se CL1 C
AC = 0.4
BC = 0.6

single link = 0.4
complete link = 0.6
average link = 0.5
```

Il **top-down clustering** fa l'opposto.

Si può utilizzare una soluzione flat (es. k-means) e applicare ricorsivamente gli elementi contenuti in ogni cluster.

Labeling del clustering è un argomemnto vedremo in futuro.


Implementare il Clustering gerarchico lato codice.

L'aspetto più complesso è identificare la struttura dati adatta per contenere il dendogramma



#### Implementazione in SciPy

```py
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendogram, linkage, cophenet
import numpy as np

z = linkage(M, 'ward') # dove linkage prende la matrice originali
print z
```

La struttura della matrice risultante ha quattro colonne :
- i primi due sono la posizione nella matrice originaria della relazione tra i due documenti
- numero di oggetti clusterizzati
- distanza tra i due oggetti


obj 1   | obj 2  | n Cluster | distanza tra i due oggetti
--------|--------|-----------|---------
        |        |           |

```
supponiamo di avere 4 risorse:
A, B, C, D
0, 1, 2, 3


prima esecuzione dell'algoritmo AB

linkage matrix

0  | 1  | deltai | 2

seconda esecuzione dell'algoritmo CD

2 |  3  | delta2 | 4

terza esecuzione dell'algoritmo AB + CD

n_elementi + indice della formazione del primo elemento 0 | n_elementi + indice della formazione del secondo elemento 0 | delta 3 | 2


Esempio diverso

o1 | o2 | d | #c
---|---|---|---
0 | 1 | a | 1
4 | 2 | b | 3
5 | 3 | c | 4


per leggere risalgo ogni radice fino alle foglie (o1, o2 <= n)
```

In questo caso utilizzare l'RSS per valutare la qualità del clustering, ma possiamo anche utilizzare la **cophenetic correlation**.

L'obiettivo è di verificare in che misura sia rispettata la distanza che abbiamo nei singoli dati.

- d(i,j) la distanza tra du bunti qualsiasi nello spazio
- t(i,j) distanza dendogrammatica dell'altezza dell'albero più basso che unisce i due punti. Tra due punti corrisponde a d ma non tra due cluster a seconda del linkage che uso
- d^ e t^ la distanza la media delle due precedenti

Voglio sapere se sono correlate d e t che si misura con la covarianza.

Varianza => (xi - xmedio)^2/(N-d) dove d è il grado di libertà della varianza che serve per limitare l'errore dato dall'utilizzo di un solo campione

Covarianza => (xi - xmedio)(yi - ymedio)/N

se il segno è positivo in un punto, significa che complessivamente hanno un comportamento uniforme

Come scelgo il treshold per tagliare il dendogramma:

- definisco una soglia manuale
  - rigenero il dendogramma in truncate_mode e definisco un treshold di colori
- automatic approach
  - scipy usa come treshold `0.7 * max(Z[:,2])`, quindi la distanza massima * 0.7
-inconsistency
  - costruire una matrice di inconsistenza (incoerenza). Prendiamo la dendogrammatic distance di ogni cluster, ne calcolo la varianzia (la media delle altezze) e la normalizzo rispetto alla standard deviation delle righe precedenti. Ovvero, terza colonna rispetto alla media, deviazione standard di tutti gli elementi precedenti nella linkage matrix. In questo modo si vede con che grado di consistenza sono state aggregate le risorse
  - definisco una soglia legata all'inconsistenza

```py
import scipy.cluster.hierarchy import inconsistent, maxinconsts

depth = len(H['ward'])
icons = inconsistent(h['ward'], depth)
```

- cophenetic distance
- maxclust: distanza cofenetica rispetta r e non forma più di k cluster
- elbow
  - trovare il gomito della curva dell'inconsistenza
  - non voglio il punto in cui la distanza accresce ma quella in cui accelera

Prendo la distribuzione delle distanze, farne la derivata seconda e prendere il punto di massimo. x numero di osservazioni, y è la soglia distanza (sommo 2 per avere il numero di 4)


Problemi di separazione lineare o non lineare

Dimension reduction

ruoto la mia matrice per un vettore che mi ruota lo spazio:
ad es. Matrice * vettore lambda = v scalare
Più grande è v autovalore, più è significativo l'impatto dell'autovettore.

Riusciamo a trovarli solo per le matrice quadrate.

Non ha molto senso applicarlo ai dati. eigenvector eigenvalue
Ordiniamo gli autovalori più grossi e valutiamo solo i più significativi
