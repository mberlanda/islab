# Incontro ISLAB 2017-04-27


### Introduzione

Il clustering va bene per trovare riscontro nei k cluster attesi e valutarne l'omogenietà.

Il **topic modeling** ci consente di partire da una situazione in cui non sappiamo esattamente quali sno le categorie attese.

La famiglia di algoritmi può essere classificata in:

- metodi algebrici: algebra lineare, si basano sull'idea di fattorizzare la matrice tag/documento
- metodi probabilistici

Problemi relativi ai **Vector Space Models**:

- _altà dimensionalità_: ridurre il numero di features da considerare per fare una valutazion
- _sinonimia_: permettono di individuare documenti che esprimono lo stesso concetto in termini di sinonimi anche quando non hanno termini in comune. Termini usati in modo analogo pure non avendo lo stesso significato linguistico.

Termini polisemici mettono in difficoltà questo tipo di algoritmi.

Abbiamo variabili non latenti (es. tf, idf etc.) e variabili latenti non esplicitamente visibili (concetti o topic che possono ritradurre i vari termini dei documenti).

### Latent Semantic Indexing o Latent Semantic Analysis

Indexing: individuare topic o concetti latenti sulla base di una terminologia non presente nella query (es. medico e chirurgo)

Analysis: stessa attività ma con finalità diverse (individuare i topic)


_Recap su eigenvalues e eigenvectors_

Sia A una matrice `n x n` di numeri reali.

Se x e un vettore n-dimensionale, allora la matrice-vettore prodotto Ax è definita e il risulato è nuovamente un vettore n-dimensionale.

In generale, la moltiplicazione per una matrice cambia la direzione di un vettore x non-zero a meno che il vettore sia speciale e abbiamo

```
Ax = Lx
dove:
L è un eigenvalue
x è un eigenvector
```

C'è da studiare un pochino di algegra lineare per matrici, autovettori, auto

Diagonalizzare la matrice

Vogliamo scomporre la matrice in 3 fattori: i concetti, la rappresentazione dei documenti in funzione dei concetti, i valir corrispondenti ai valori

AX = autovettori per autovalori
AX = AL
A  = XLX-1 se la matrice è invertibile e quadrata

Per le matrici simmetriche
A = trasposta A
A = XLXt


In soldoni:

- moltiplico la matrice A per la trasposta At = B
- B è quadrata e simmetrica
- Estraggo gli autovalori da B
- Gli autovettori che mi interessano sono
  - quelli di A x1 .. xk
  - gli autovettori di B (quelli che descriveranno i documenti)
    - li ordino per dimensione (tutti positivi perché simmetrica)
    - posso scalarli perché sono descritti come vettori unitari
    - prendo la radice quadrata degli autovalori (perché tanto nei passaggi successivi li moltiplico due volte)
      - s^2i = li
    - definisco la matrice Yi = [tk= 1/si * Axi]
    - tutti i vettori che produco sono ortogonali (perpendicolari) e di lunghezza unitaria
    - posso riscrivere tutti i componenti della matrice

U è k x m (topic per termini), Sigma i singluar values , V n x k

Singular value decomposition è una tecnica di topic modeling.

sigma rappresenta i singular values
