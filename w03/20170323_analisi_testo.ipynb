{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incontro ISLAB 2017-03-23 | Analisi del testo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prima di cominciare\n",
    "\n",
    "- Per prima cosa bisogna considerare cosa definiamo per testo e quali processi di normalizzazione sono necessari.\n",
    "- Definire la gerarchia/granularità da aggiungere come metadati (es. verso, strofe, poesia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizzazione\n",
    "\n",
    "- Alcune operazioni di tokenizzazione dipendono dalla lingua del testo: caratteri speciali, punteggiatura\n",
    "- In alcuni casi bisogna trattare il dominio (hashtag in un tweet invece che in un altro contesto)\n",
    "- Fenomeni morfologici: trattini per parole composte\n",
    "- Problemi di encoding\n",
    "\n",
    "Es. separazione con gli spazi:\n",
    "\n",
    "- maiuscole e minuscole: \"Quel ramo del lago di Como\" -> non posso sempre eliminarle\n",
    "- punteggiatura: non ha senso all'interno di un token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python esiste `nlt` che contiene diversi **tokenizer**.\n",
    "Quando uso una regex, devo indicare tutto ciò che voglio catturare.\n",
    "\n",
    "Una volta ottenuti dei token, possiamo seguire diversi approcci:\n",
    "\n",
    "- mantenere tutto (maiuscole, minuscole, stopwords etc.) -> costruisco un dizionario enorme (motori di ricerca)\n",
    "- applicare dei filtri\n",
    "\n",
    "Esistono molti modi per capire in quale lingua è scritto il testo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def detect_language(text_or_tokens, tokenize=True):\n",
    "    languages_rations = {}\n",
    "    lang = 'english'\n",
    "    if tokenize:\n",
    "        tokens = tokenizer.wordpunct_tokenize(text_or_tokens)\n",
    "        words = [x.lower() for x in tokens]\n",
    "    else:\n",
    "        words = text_or_tokens\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set([x.decode('utf-8').encode('utf-8') for x in stopwords.words(language)])\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "    choice = sorted(languages_ratios.itmes(), key=lambda x: -x[0])\n",
    "    # mancano poche righe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due modi per definire le stopwords:\n",
    "\n",
    "- liste predefinite (es. nltk)\n",
    "- lista custom (lista di dominio, oppure possiamo usare l'idf per tagliare le parole meno significative) -> idf basso\n",
    "\n",
    "Come tagliamo questa distribuzione?\n",
    "\n",
    "- definiamo una soglia statica a priori: non tiene conto della distribuzione\n",
    "- posso decidere di prendere un certo percentile (max 5%) -> usare un corpus generico rappresentativo della lingua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le parole hanno una morfologia che cambia anche quando non cambia il significato:\n",
    "\n",
    "- plurale/singolare\n",
    "- coniugazione dei verbi\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "- riduciamo la dimensione del dizionario\n",
    "- si individua con maggior efficacia la semantica\n",
    "\n",
    "Svantaggi:\n",
    "- si perde la visiblità degli aspetti sintattici\n",
    "\n",
    "Criticità:\n",
    "\n",
    "- La negazione pone molti problemi\n",
    "- Ordine delle parole -> posting list dell'offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Normalizzazione puramente sintattica. Ricondurre le parole a una forma unica codifica che non è una radice della parola. Es. le parole `query` e `querying` diventano `queri`.\n",
    "\n",
    "Tutti questi meccanismi sono language dependent. Si eliminano suffissi comuni e si riconducono a una parola normalizzata.\n",
    "\n",
    "Questa operazione non è reversibile (conviene salvarsi un risultato) e potrebbe riconciliare parole che non hanno lo stesso significato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'play', u'play', 'is', u'are', u'caress', u'poni', u'poni']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "tokens = ['play', 'playing', 'is', 'are', 'caresses', 'ponies', 'pony']\n",
    "stemmer = SnowballStemmer('english')\n",
    "print [stemmer.stem(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Questa operazione si fa con qualche tipo di thesaurus (es. wordnet). Si cerca di ricondurre alla voce del dizionario.\n",
    "\n",
    "Ha due problemi:\n",
    "\n",
    "- non ha senso usare wordnet senza avere il sinset della parola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "n\n",
      "[u'play', u'drama', u'dramatic_play']\n",
      "[u'dramatic_composition', u'dramatic_work']\n",
      "playing\n",
      "n\n",
      "[u'playing']\n",
      "[u'musical_performance']\n",
      "is\n",
      "v\n",
      "[u'be']\n",
      "are\n",
      "n\n",
      "[u'are', u'ar']\n",
      "[u'area_unit', u'square_measure']\n",
      "caresses\n",
      "n\n",
      "[u'caress']\n",
      "[u'stroke', u'stroking']\n",
      "ponies\n",
      "n\n",
      "[u'pony']\n",
      "[u'horse', u'Equus_caballus']\n",
      "pony\n",
      "n\n",
      "[u'pony']\n",
      "[u'horse', u'Equus_caballus']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "tokens = ['play', 'playing', 'is', 'are', 'caresses', 'ponies', 'pony']\n",
    "for t in tokens:\n",
    "    print t\n",
    "    synsets = wn.synsets(t)\n",
    "    for s0 in synsets[0:1]:\n",
    "        print s0.pos()\n",
    "        print [l.name() for l in s0.lemmas()]\n",
    "        for h in s0.hypernyms():\n",
    "            print [l.name() for l in h.lemmas()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrase Queries\n",
    "\n",
    "**Biwords** (or phrase) indexes\n",
    "due o più parole consecutive\n",
    "es. New York, Don Abbondio\n",
    "\n",
    "- Possiamo indicizzarle a parte\n",
    "- Sostituiamo le voci\n",
    "\n",
    "Un caso generale è quello di co-occorenza: non ci interessa che compaiano consecutivamente, ma che siano compresenti nello stesso testo.\n",
    "Si definisce una finestra nel testo entro cui valutare la co-occorrenza.\n",
    "\n",
    "- indicizziamo tutte le coppie di parole\n",
    "- vediamo le ripezioni di ogni coppia\n",
    "- frequenza relativa rispetto alla parola (modo corretto di stimare la probabilità)\n",
    "- vedere quanto spesso compaiono insieme rispetto alla occorrenze totali di uno dei due termini\n",
    "\n",
    "$p(ab)_{attesa} = p(a) * p(b)$\n",
    "\n",
    "[mutual information](https://en.wikipedia.org/wiki/Mutual_information) as relevance of a 2-word\n",
    "\n",
    "$m(x, y) = p(x,y) log(p(x,y)/(p(x)*p(y)))\n",
    "\n",
    "[https://books.google.com/ngrams](https://books.google.com/ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permuterm Index\n",
    "\n",
    "#### Spelling correction\n",
    "\n",
    "#### Phonetic correction\n",
    "\n",
    "es. soundex algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
