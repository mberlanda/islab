# Incontro ISLAB 2017-03-30


### Vector space model

Possiamo orientare nello spazio dei documenti i vettori dello parole

- Term frequency
- Document frequency
- Tf*Idf per una migliore rilevazione della specificità

Bag of words:

abbiamo un documento (ciascuna immagini) e tutti i termini presenti in un corpus (i tag).
Dovremmo riportare per ogni documento un vettore con tutti i termini.
Se facessimo così avremmo una matrice estremamente sparsa perché moltisimi vettori sarebbero a 0.
In alternativa consideriamo solo le posizioni <> 0 all'interno del vettore.
Tag presenti => Score di ciascun tag

La bag of words può essere considerata come la proiezione delle colonne che vogliamo considerare

`['need', 'love', 'play']`

doc id | title | weights
-------|-------|--------
11111  | bla bla| [0.1,0.3,0.5]
11112  | bla blo| [0.7,0.4,0.8]
11113  | bla blo| [0.9,0.2,0.0]

Stiamo però modificando la struttura di un documento perché ne prendiamo solo un aspetto parziale.

Vector space model:

Dato un dizionario D di dimensione D, definiamo uno spazio multidimensionale con D dimensioni, dove la i-esima dimensione è corrispondente all'i-esimo termine del documento

I pesi rappresentano le coordinate del vettore che identifica il documento.
Questi oggetti sono indicativi della direzione che l'oggetto ha nello spazio.
Vengono fuori con una lunghezza diversa? I numeri non sono molto comparabili, perché i pesi sono indipendenti tra di loro.

Esiste un metodo per trasformazione in vettori unitari, normalizzati rispetto alla lunghezza.

Unit Vectors:
v = V / (sqr(sumi vi ** 2))  // norma euclidea, lunghezza euclidea

Manteniamo la proprozione tra i numeri in modo tale che la somma sia uguale a 1.


Adesso che tutti i vettori hanno la stessa lunghezza è più semplice definire la similarità tra i gruppi.
La similarità può essere misurata con la dimensione dell'angolo tra i vari vettori.

la funzione di similarità che usiamo è la cosine similariy

sim(d1,d2) = vd1 x vd2 / (sqrt(norm.vd1)*sqrt(norm.vd2))

dot product = d1i * d2i, d1ii + d2ii


dati due modelli:

bag of words contiene solo i tag presenti, quindi possiamo limitarci all'intersezione dei termini


Cluster pruning:

Si scelgono dei leader che possono essere:

- documenti reali particolarmente rappresentativi
- centroidi di un cluster di documenti
- documenti virtuali con delle dimensioni e dei pesi che più ci interessano

Nel nostro caso possiamo valutare l'omogeneità dei tag per utilizzare dei vettori che siano indicatori di un'area semantica.
Calcoliamo quindi la similarità tra tag invece delle similarità dei documenti.
Quindi il nostro documento sarà il tag e il vettore composto dai documenti.

In information retrieval si applica query expansion prima di eseguire la query.
Si utilizzano due metodologie principali:

- locali: relevance feedback (da parte degli utenti, es. la selezione cliccata dall'utente), pseudo-relevance feedback, indirect feedback
- globali: thesaurs ( e.g. WordNet), automatic thesaurus generation, spelling correction

Rocchio Algorithm (con feedback):

data una query q e il relevance feedback dato da revelant documents Cr and non relevant documents Cnr, wvogliamo trovare un vettore di query ottimizzato q0 tale che:
  q0 = argmaxq ( sim(q,Cr) - sim(q,Cnr))

Nel vector space model usando cosin similarity abbiamo:
 q0 = aq + ...

Co-Occurrence Thesaurus
create una matrice di term-document F dove Fij rappresenta il numero di occorrenze del termine i nel documento j

e.g.

normalizzo rispetto ad ogni riga [2, 2, 1, 0] [1, 1, 0.5, 0]

_ | 0 | 1 | 2
---|---|---|---
A | 1.0 | |
B | 1.0 | |
C | 0.57 | |
D | 0.0 | |

moltiplico la matrice per la trasposta e ottengo una matrice di termini termini di cui riesco ad avere la similitudine


Similitudine:

- Cosine distance
- Euclidean distance -> basata sui quadrati del modulo della differenza dei vettori
- Chebyshev distance -> distanza massima (utilizzato per ridurre le dimensioni)
- Manhattan distance -> distanza intesa su uno spazio a forma di griglia come Manhattan (no pitagora ma solo cateti)
- Canberra distance -> Manhattan distance normalizzata (prodotto dei moduli)
- Bray-Cyrtis distance -> Manhattan distance normalizzata in un altro modo (sommatoria del modulo della somma degli elementi)
- Correlation distance -> due vettori, calcolo il vettore medio, shifto rispetto alla media
- Minkowski distance -> è una generalizzazione di tutte le distanze basate sul modulo con un parametro p (p1 manhattan, p2 euclidean, etc.). Se p tende a infinito tengo Chebyshev distance

Solitamente si usa la distanza e non la similarità